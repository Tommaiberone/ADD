##############################################
#	Description of the Method Implemented    #
##############################################


Our code is an implementation of Adversarial Diffiusion Distillation. The basic idea is to train a Diffiusion Model to achieve good performance
with less diffusion steps, with a GAN-like training procedure.
This architecture consists of:

	-	The Generator (or Student) Model, which generates an image in 4 diffusion steps.
	-	The Teacher model, which generates an image in 100 diffusion steps.
	-	The Discriminator, which classifies if the image produced by the Generator is a real image or a generated one.
	
The training procedure is structured in the following way:

	-	An image with its caption is retrived from its url stored in the dataset. 
	-	The Discriminator classifies if this image from the dataset is real or not.
	-	The real image is diffused and transformed into white gaussian noise.
	-	The diffused image and its caption are inputted into the Generator and a new image is
		created in four diffusion steps.
	-	The Discriminator classifies if the new image is real or not.
	-	The new image is diffused and it's transformed into white noise.
	-	The new diffused image and its captions are inputted to the Teacher model and an other image
		is generated in 100 diffusion steps.
		
We use different loss function to train the various models:

	-	The Discriminator is trained on the average Binary Cross Entropy between the result of te classification of the true 
		and generated imagines.
	-	The loss of the Student Model instead is composed by two factors:
			-	The Adversarial loss, that is the BCE between the prediction of the Discriminator for the generated image and the ground true label.
			-	The Distillation loss, that is a Mean Squared Error between the image generated by the Teacher model and the one generated by the 
				Student model.
				
During training the Distilletion loss is multiplied by a  factor l of 5, in order to make it of the same magnitude of the Adversarial loss.

###########################
#	How to Run the Code   #
###########################

All the libraries needed to run the code are installed through the pip command at runtime in the first code block of the notebook.
The pretrained models are to be retrieved at runtime from huggingface.com, thus an internet connection is required
to run the code.
The dataset is included in the repository with the name "dataset.json", but the images are retrieved at runtime from their 
URL, once again needing an internet connection.
The notebook last execution is saved in the repository, thus it contains the execution of each cell (besides the training loop one,
which would have contained 768 images, so it has been cut for github storing limitations).



		
