#####################
#	Paper Review    #
#####################

Diffusion models like stable diffusion have reached state of the art capabilities in generating images that are almost indistinguishable from human produced ones. Nevertheless, they are computationally very heavy on the hardware and require hundreds or thousands of diffusion steps in order to produce good results. This iterative inference process in DMs currently hinders their real-time application due to the number of steps required.

Generative Adversarial Networks (GANs), on the other hand, are characterized by their single-step formulation and inherent speed. Often though, the results produced by GAN training arenâ€™t comparable to the DM ones.

Adversarial Diffusion Distillation (ADD) is a new training approach that constitutes a huge step forward in the context of generative models. It combines the superior sample quality of DMs with the inherent speed of GANs.

The idea is to distill the knowledge of a Diffusion teacher model operating in 1000 steps into a Diffusion student model that operates in just 1-4 steps. This score distillation, using existing image diffusion models as a teaching signal, is combined with an adversarial loss to ensure fidelity in low-step sampling.

ADD surpasses existing few-step methods like GANs and Latent Consistency Models in a single step and equals the performance of SDXL in just four steps, marking a significant advancement in single-step, real-time image synthesis with foundation models.


##############################################
#	Description of the Method Implemented    #
##############################################


Our code is an implementation of Adversarial Diffiusion Distillation. The basic idea is to finetune a Diffusion Model in order to achieve comparable performances with less diffusion steps, leveraging a GAN-like training procedure.


This architecture consists of three pretrained Models:

	-	The Generator (or Student) Model, which generates an image in 4 diffusion steps.
	-	The Teacher model, which generates an image in 100 diffusion steps.
	-	The Discriminator, which classifies if the image produced by the Generator is a real image or a generated one.
	
The training procedure is structured in the following way:

	-	An image with its caption is retrieved from its url stored in the dataset. 
	-	The Discriminator classifies this image from the dataset as real or generated.
	-	The real image is diffused and transformed into white gaussian noise.
	-	The diffused noise and its caption are fed to the Generator and a new image is
		created in four diffusion steps.
	-	The Discriminator classifies the new image as real or generated.
	-	The new image is diffused and is transformed again into white noise.
	-	The new diffused noise and its captions are fed to the Teacher model and an other image
		is generated in 100 diffusion steps.
	-	The losses are computed and the gradient descent is performed on both the Student and the discriminator models
		
We use different loss functions to train the two models:

	-	The Discriminator is trained on the average Binary Cross Entropy among the results of the classification of the true generated images.
	-	The loss of the Student Model instead is the sum of two components:
			-	The Adversarial loss, that is the BCE between the prediction of the Discriminator for the generated image and the ground true label.
			-	The Distillation loss, that is a Mean Squared Error between the image generated by the Teacher model and the one generated by the Student model.
				
During training the Distilletion loss is multiplied by a factor l that is set to 5, in order to make it of the same magnitude of the Adversarial loss.


###############
#	Dataset   #
###############

To train our student model we used a subset of the COCO 2017 Dataset. All the images are retrieved one by one from their URL at training time for practical reasons. All considered, the time necessary for the teacher model to produce the model outweighs the time needed to retrieve the image.
The COCO (Common Objects in Context) Dataset for Image Captioning is a comprehensive collection that contains over 200,000 labeled images, spanning a diverse range of 80 object categories and over 1 million object instances. These images are rich in everyday scenes, containing common objects in their natural context.


###########################
#	How to Run the Code   #
###########################

All the libraries needed to run the code are installed through the pip command at runtime in the first code block of the notebook.
The pretrained models are to be retrieved at runtime from huggingface.com, thus an internet connection is required to run the code.
The dataset is included in the repository with the name "dataset.json", but the images are retrieved at runtime from their URL, once again needing an internet connection.
The notebook last execution is saved in the repository, thus it contains the execution of each cell (besides the training loop one,
which would have contained 768 images, so it has been cut for github storing limitations).



		
